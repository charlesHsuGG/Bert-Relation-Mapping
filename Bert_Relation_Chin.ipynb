{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert-Relation-Chin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesHsuGG/Bert-Relation-Mapping/blob/master/Bert_Relation_Chin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Epo4apkwtG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "f8f9a5eb-ba7f-4483-d4a7-34b4cb715f8c"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_cased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 06:34:52--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.194.128, 2607:f8b0:4001:c14::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.194.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1242589256 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_cased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "wwm_cased_L-24_H-10 100%[===================>]   1.16G   183MB/s    in 6.4s    \n",
            "\n",
            "2020-02-06 06:34:59 (185 MB/s) - ‘wwm_cased_L-24_H-1024_A-16.zip’ saved [1242589256/1242589256]\n",
            "\n",
            "Archive:  wwm_cased_L-24_H-1024_A-16.zip\n",
            "   creating: wwm_cased_L-24_H-1024_A-16/\n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-kY6BVjt72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "58b7f0ae-6557-492a-8a88-2a75e9fb8736"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_zpRCnFseNe"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amye3Z4Tk4hE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "703ee186-8c9d-4351-996e-4cf70ebf9e5b"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoL2a41UiHva"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from tokenization import FullTokenizer     # Still from bert module\n",
        "from modeling import BertConfig, BertModel\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQeDfDXk9AG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "873cabfe-3337-46bd-f12e-7b5bf5b20780"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypo66KOqlDeG"
      },
      "source": [
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbh8wJzSlEJt"
      },
      "source": [
        "# Google Colab don't need this. FullTokenizer is not updated to tf2.0 yet\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9w-4hRCvDBW"
      },
      "source": [
        "MODEL_DIR = \"wwm_cased_L-24_H-1024_A-16\" #@param {type:\"string\"} ['wwm_cased_L-24_H-1024_A-16']\n",
        "\n",
        "config_path = \"/content/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "bert_config = BertConfig.from_json_file(config_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr0flmssiPw9"
      },
      "source": [
        "class RelationTokebizer(FullTokenizer):\n",
        "\n",
        "  def tokenize(self, text):\n",
        "      split_tokens = []\n",
        "      for sub_text in re.split(\"(<e[1-9]1>|<e[1-9]2>)\", text):\n",
        "        if re.findall(\"^(<e[1-9]1>|<e[1-9]2>)\", sub_text):\n",
        "          split_tokens.append(sub_text)\n",
        "        else:\n",
        "          for token in self.basic_tokenizer.tokenize(sub_text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "              if sub_token in self.vocab:\n",
        "                split_tokens.append(sub_token)\n",
        "              elif _is_whitespace(sub_token):\n",
        "                split_tokens.append(\"[unused1]\")\n",
        "              else:\n",
        "                split_tokens.append(\"[UNK]\")\n",
        "\n",
        "      return split_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLnUSEEMr2bW"
      },
      "source": [
        "tokenizer = RelationTokebizer(vocab_path, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edwo3W50rkEO"
      },
      "source": [
        "**建立切詞器**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIU4H519vhTE"
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdZ4r2w7BBML"
      },
      "source": [
        "def relation_map():\n",
        "    relation_map = {\"UNKNOW\":0}\n",
        "    data_path = \"/content/label.txt\"\n",
        "    with open(data_path, 'r') as f:\n",
        "      for line in f:\n",
        "          name = line.replace(\"\\n\", \"\")\n",
        "          relation_map[name] = len(relation_map)\n",
        "      return relation_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laCZfLX0IPwQ"
      },
      "source": [
        "relation_map = relation_map()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbnClONLrSuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "d2fcdcc4-d4a9-4e43-ece2-6703e9f3f04f"
      },
      "source": [
        "relation_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cause-Effect(e1,e2)': 2,\n",
              " 'Cause-Effect(e2,e1)': 3,\n",
              " 'Component-Whole(e1,e2)': 14,\n",
              " 'Component-Whole(e2,e1)': 15,\n",
              " 'Content-Container(e1,e2)': 8,\n",
              " 'Content-Container(e2,e1)': 9,\n",
              " 'Entity-Destination(e1,e2)': 12,\n",
              " 'Entity-Destination(e2,e1)': 13,\n",
              " 'Entity-Origin(e1,e2)': 10,\n",
              " 'Entity-Origin(e2,e1)': 11,\n",
              " 'Instrument-Agency(e1,e2)': 4,\n",
              " 'Instrument-Agency(e2,e1)': 5,\n",
              " 'Member-Collection(e1,e2)': 16,\n",
              " 'Member-Collection(e2,e1)': 17,\n",
              " 'Message-Topic(e1,e2)': 18,\n",
              " 'Message-Topic(e2,e1)': 19,\n",
              " 'Other': 1,\n",
              " 'Product-Producer(e1,e2)': 6,\n",
              " 'Product-Producer(e2,e1)': 7,\n",
              " 'UNKNOW': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv0qZMZ79tzN"
      },
      "source": [
        "relation_type_num = len(relation_map)\n",
        "max_seq_length=384"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuIIc8Isegp2"
      },
      "source": [
        "class RBERT(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, doropout_rate = 0.1, max_seq_length=max_seq_length, bert_config = bert_config, hidden_size=768, num_labels=relation_type_num):\n",
        "    super(RBERT, self).__init__()\n",
        "    self.input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "    self.input_word_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"input_word_mask\")\n",
        "    self.segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                        name=\"segment_ids\")\n",
        "    self.e1_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                        name=\"e1_mask\")\n",
        "    self.e2_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                        name=\"e2_mask\")\n",
        "    \n",
        "    self.bert_layer = BertModel(config=bert_config, name='bert')\n",
        "\n",
        "    self.cls_fc_droput = tf.keras.layers.Dropout(doropout_rate)\n",
        "    self.cls_fc_layer = tf.keras.layers.Dense(hidden_size, activation=\"tanh\")\n",
        "    self.e1_fc_droput = tf.keras.layers.Dropout(doropout_rate)\n",
        "    self.e1_fc_layer = tf.keras.layers.Dense(hidden_size, activation=\"tanh\")\n",
        "    self.e2_fc_droput = tf.keras.layers.Dropout(doropout_rate)\n",
        "    self.e2_fc_layer = tf.keras.layers.Dense(hidden_size, activation=\"tanh\")\n",
        "    \n",
        "    self.concat_h = tf.keras.layers.Concatenate(axis=-1)\n",
        "\n",
        "    self.logit_output = tf.keras.layers.Dropout(doropout_rate)\n",
        "    self.logit = tf.keras.layers.Dense(num_labels, activation=\"softmax\")\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, inputs, build = False):\n",
        "    input_word_ids, input_mask, segment_ids, e1_mask_input, e2_mask_input = inputs\n",
        "    if not build:\n",
        "      input_word_ids = self.input_word_ids(input_word_ids)\n",
        "      input_mask = self.input_word_mask(attention_mask)\n",
        "      segment_ids = self.segment_ids(token_type_ids)\n",
        "      e1_mask_input = self.e1_mask(e1_mask)\n",
        "      e2_mask_input = self.e2_mask(e2_mask)\n",
        "    pooled_output, sequence_output = self.bert_layer(input_word_ids=input_word_ids,\n",
        "                                                      input_mask=input_mask,\n",
        "                                                      input_type_ids=segment_ids)\n",
        "    e1_h = self.entity_average(sequence_output, e1_mask_input)\n",
        "    e2_h = self.entity_average(sequence_output, e2_mask_input)\n",
        "    print(e1_h)\n",
        "    cls_dropout = self.cls_fc_droput(pooled_output)\n",
        "    pooled_output = self.cls_fc_layer(cls_dropout)\n",
        "\n",
        "    e1_dropout = self.e1_fc_droput(e1_h)\n",
        "    e1_h  = self.e1_fc_layer(e1_dropout)\n",
        "\n",
        "    e2_dropout = self.e2_fc_droput(e2_h)\n",
        "    e2_h  = self.e2_fc_layer(e2_dropout)\n",
        "\n",
        "    concat_h = self.concat_h([pooled_output, e1_h, e2_h])\n",
        "\n",
        "    drop_output = self.logit_output(concat_h)\n",
        "    logits = self.logit(drop_output)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def buile_model(self, max_seq_length=max_seq_length):\n",
        "      input_ids_shape=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
        "      attention_mask_shape=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
        "      token_type_ids_shape=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
        "      e1_mask_shape=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
        "      e2_mask_shape=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
        "      inputs = input_ids_shape, attention_mask_shape, token_type_ids_shape, e1_mask_shape, e2_mask_shape\n",
        "      return Model(inputs=[inputs], outputs=self.call(inputs, build = True))\n",
        "\n",
        "  @staticmethod\n",
        "  def entity_average(sequence_output, e_mask):\n",
        "    e_mask_unsqueeze = tf.expand_dims(e_mask, 1)\n",
        "    length_tensor = tf.expand_dims(tf.reduce_sum(e_mask, 1), 1)\n",
        "    sum_vector = tf.squeeze(tf.matmul(tf.dtypes.cast(e_mask_unsqueeze, tf.float32), sequence_output), axis = 1)\n",
        "    avg_vector = tf.dtypes.cast(sum_vector, tf.float32) / tf.dtypes.cast(length_tensor, tf.float32)\n",
        "    return avg_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6XBRl9QSatB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "13992ca6-3973-4647-b796-a9be6a9d539e"
      },
      "source": [
        "model = RBERT().buile_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"truediv:0\", shape=(None, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cax-0bqBHTgB"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PktGRYeGhNC"
      },
      "source": [
        "model_params = {v.name:v for v in model.trainable_variables}\n",
        "model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
        "# print(model_roots)\n",
        "saved_names = [k for k,v in tf.train.list_variables('/content/wwm_cased_L-24_H-1024_A-16/bert_model.ckpt')]\n",
        "a_map = {v:v+':0' for v in saved_names}\n",
        "# print(a_map)\n",
        "model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
        "# print(model_roots)\n",
        "def transform(x):\n",
        "    x = x.replace('attention/self','attention')\n",
        "    x = x.replace('attention','self_attention')\n",
        "    x = x.replace('attention/output','attention_output')  \n",
        "\n",
        "    x = x.replace('/dense','')\n",
        "    x = x.replace('/LayerNorm','_layer_norm')\n",
        "    x = x.replace('embeddings_layer_norm','embeddings/layer_norm')  \n",
        "\n",
        "    x = x.replace('attention_output_layer_norm','attention_layer_norm')  \n",
        "    x = x.replace('embeddings/word_embeddings','word_embeddings/embeddings')\n",
        "\n",
        "    x = x.replace('/embeddings/','/embedding_postprocessor/')  \n",
        "    x = x.replace('/token_type_embeddings','/type_embeddings')  \n",
        "    x = x.replace('/pooler/','/pooler_transform/')  \n",
        "    x = x.replace('answer_type_output_bias','ans_type/bias')  \n",
        "    x = x.replace('answer_type_output_','ans_type/')\n",
        "    x = x.replace('cls/nq/output_','logits/')\n",
        "    x = x.replace('/weights','/kernel')\n",
        "\n",
        "    return x\n",
        "a_map = {k:model_params.get(transform(v),None) for k,v in a_map.items() if k.startswith('bert')}\n",
        "# print(a_map)\n",
        "tf.compat.v1.train.init_from_checkpoint(ckpt_dir_or_file='/content/wwm_cased_L-24_H-1024_A-16/bert_model.ckpt',\n",
        "                                        assignment_map=a_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_yirKTSHaF0"
      },
      "source": [
        "model.layers[7].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIi_YsXlWdpH"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5buZZhsV5R4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "113af929-dbbf-4427-ef20-2b155ec58077"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 1, 384)]     0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 384)]     0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bert (BertModel)                ((None, 1024), (None 333579264   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast (TensorFlowOpL [(None, 1, 384)]     0           tf_op_layer_ExpandDims[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum (TensorFlowOpLa [(None,)]            0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_2 (TensorFlowO [(None, 1, 384)]     0           tf_op_layer_ExpandDims_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum_1 (TensorFlowOp [(None,)]            0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatMul (TensorFlowO [(None, 1, 1024)]    0           tf_op_layer_Cast[0][0]           \n",
            "                                                                 bert[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_1 (Tenso [(None, 1)]          0           tf_op_layer_Sum[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatMul_1 (TensorFlo [(None, 1, 1024)]    0           tf_op_layer_Cast_2[0][0]         \n",
            "                                                                 bert[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 1)]          0           tf_op_layer_Sum_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 1024)]       0           tf_op_layer_MatMul[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_1 (TensorFlowO [(None, 1)]          0           tf_op_layer_ExpandDims_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None, 1024)]       0           tf_op_layer_MatMul_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_3 (TensorFlowO [(None, 1)]          0           tf_op_layer_ExpandDims_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv (TensorFlow [(None, 1024)]       0           tf_op_layer_Squeeze[0][0]        \n",
            "                                                                 tf_op_layer_Cast_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv_1 (TensorFl [(None, 1024)]       0           tf_op_layer_Squeeze_1[0][0]      \n",
            "                                                                 tf_op_layer_Cast_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           bert[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           tf_op_layer_truediv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           tf_op_layer_truediv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          787200      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 768)          787200      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 768)          787200      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 2304)         0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 2304)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 20)           46100       dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 335,986,964\n",
            "Trainable params: 2,407,700\n",
            "Non-trainable params: 333,579,264\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8iom1XnGzlI"
      },
      "source": [
        "# def insert_entity_token(sentence, entity_pair):\n",
        "#   temp_start_word = \"\"\n",
        "#   temp_end_word = \"\"\n",
        "#   sent_list = list(sentence)\n",
        "#   for i, ent in enumerate(entity_pair):\n",
        "#     index = 0\n",
        "#     temp_sent_list = []\n",
        "#     for word in sent_list:\n",
        "#         if index == ent['start_ind'] and index == ent['end_ind'] - 1:\n",
        "#            temp_sent_list.append(\"<e\"+str(i+1)+\"1>\")\n",
        "#            temp_sent_list.append(word)\n",
        "#            temp_sent_list.append(\"<e\"+str(i+1)+\"2>\")\n",
        "#         elif index == ent['start_ind']:\n",
        "#            temp_sent_list.append(\"<e\"+str(i+1)+\"1>\")\n",
        "#            temp_sent_list.append(word)\n",
        "#         elif index == ent['end_ind'] - 1:\n",
        "#            temp_sent_list.append(word)\n",
        "#            temp_sent_list.append(\"<e\"+str(i+1)+\"2>\")\n",
        "#         else:\n",
        "#            temp_sent_list.append(word)\n",
        "#         if temp_start_word and temp_end_word:\n",
        "#            if word != temp_start_word or word != temp_end_word:\n",
        "#               index += 1\n",
        "#         else:\n",
        "#            index += 1\n",
        "#     # print(temp_sent_list)\n",
        "#     sent_list = temp_sent_list\n",
        "#     temp_start_word = \"<e\"+str(i+1)+\"1>\"\n",
        "#     temp_end_word = \"<e\"+str(i+1)+\"2>\"\n",
        "#   return \"\".join(sent_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYj2n2owAVoC"
      },
      "source": [
        "def convert_relation_type(type_name):\n",
        "    return relation_map[type_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIyI0U9cmbKG"
      },
      "source": [
        "def convert_to_feature(sentence, max_seq_length = max_seq_length):\n",
        "    \"\"\"\n",
        "    convert to feature\n",
        "    \"\"\"\n",
        "    sentence = sentence.replace(\"<e1>\", \"<e11>\").replace(\"</e1>\", \"<e12>\").replace(\"<e2>\", \"<e21>\").replace(\"</e2>\", \"<e22>\")\n",
        "    tokens_a = tokenizer.tokenize(sentence)\n",
        "\n",
        "    if \"<e22>\" not in tokens_a:\n",
        "      print(tokens_a)\n",
        "    e11_p = tokens_a.index(\"<e11>\")  # the start position of entity1\n",
        "    e12_p = tokens_a.index(\"<e12>\")  # the end position of entity1\n",
        "    e21_p = tokens_a.index(\"<e21>\")  # the start position of entity2\n",
        "    e22_p = tokens_a.index(\"<e22>\")  # the end position of entity2\n",
        "\n",
        "    tokens_a[e11_p] = \"$\"\n",
        "    tokens_a[e12_p] = \"$\"\n",
        "    tokens_a[e21_p] = \"#\"\n",
        "    tokens_a[e22_p] = \"#\"\n",
        "\n",
        "    e11_p += 1\n",
        "    e12_p += 1\n",
        "    e21_p += 1\n",
        "    e22_p += 1\n",
        "\n",
        "    special_tokens_count = 2 #### for bert\n",
        "    if len(tokens_a) > max_seq_length - special_tokens_count:\n",
        "        tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
        "    # print(tokens_a)\n",
        "    stokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "    input_word_ids = get_ids(stokens, tokenizer, max_seq_length=max_seq_length)\n",
        "    input_mask = get_masks(stokens, max_seq_length=max_seq_length)\n",
        "    segment_ids = get_segments(stokens, max_seq_length=max_seq_length)\n",
        "    \n",
        "    e1_mask = [0] * len(input_mask)\n",
        "    e2_mask = [0] * len(input_mask)\n",
        "\n",
        "    for i in range(e11_p, e12_p):\n",
        "        e1_mask[i] = 1\n",
        "    for i in range(e21_p, e22_p):\n",
        "        e2_mask[i] = 1\n",
        "\n",
        "    assert len(input_word_ids) == max_seq_length, \"Error with input length {} vs {}\".format(len(input_word_ids), max_seq_length)\n",
        "    assert len(input_mask) == max_seq_length, \"Error with attention mask length {} vs {}\".format(len(input_mask), max_seq_length)\n",
        "    assert len(segment_ids) == max_seq_length, \"Error with token type length {} vs {}\".format(len(segment_ids), max_seq_length)\n",
        "    \n",
        "    return input_word_ids, input_mask, segment_ids, e1_mask, e2_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e36YoFyV-N7O"
      },
      "source": [
        "def data_generator(data_path = \"/content/train.tsv\"):\n",
        "    with open(data_path, 'r') as f:\n",
        "      for line in f:\n",
        "        relation_type, sent = line.split('\\t')\n",
        "        # print(entity_a, entity_b)\n",
        "        input_features = convert_to_feature(sent)\n",
        "        # print(relation_type)\n",
        "        label_id = convert_relation_type(relation_type)\n",
        "        yield input_features, label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa6wGc8_Buy_"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_generator(data_generator, ((tf.int32, tf.int32, tf.int32, tf.int32, tf.int32), (tf.int32)), args=(\"/content/train.tsv\",))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HGcERcVIkcF"
      },
      "source": [
        "test_ds = tf.data.Dataset.from_generator(data_generator, ((tf.int32, tf.int32, tf.int32, tf.int32, tf.int32), (tf.int32)), args=(\"/content/test.tsv\",))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4t315DEJ8I1"
      },
      "source": [
        "TRAIN_DATA_SIZE = len([ds for ds in train_ds])\n",
        "TEST_DATA_SIZE = len([ds for ds in test_ds])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBn9jVdVKUeo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "075a2004-afa2-49cc-804a-139d0c9699b7"
      },
      "source": [
        "print(TRAIN_DATA_SIZE)\n",
        "print(TEST_DATA_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "2717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXwfR7MaTRlF"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_4qcS22qMA1"
      },
      "source": [
        "train_size = int(0.7 * TRAIN_DATA_SIZE)\n",
        "val_size = int(0.3 * TRAIN_DATA_SIZE)\n",
        "\n",
        "\n",
        "full_dataset = train_ds.shuffle(10, reshuffle_each_iteration = False )\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "test_dataset = full_dataset.skip(train_size)\n",
        "val_dataset = test_dataset.take(val_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d1EolsHTEVH"
      },
      "source": [
        "train_dataset1 = train_dataset.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDByZa_TUjE"
      },
      "source": [
        "def generator():\n",
        "    while True:\n",
        "        print(\"initialize DataSet\")\n",
        "        for inputs, output in train_dataset1:\n",
        "            yield inputs, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAxyj_9eTXCM"
      },
      "source": [
        "val_dataset1 = val_dataset.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgJpLZN8TdM1"
      },
      "source": [
        "def vaild_generator():\n",
        "    while True:\n",
        "      print(\"initialize DataSet\")\n",
        "      for inputs, output in val_dataset1:\n",
        "          yield inputs, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up5kI-FmTfw9"
      },
      "source": [
        "el_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = 'model_zero7.{epoch:02d}-{loss:.6f}.hdf5',\n",
        "                                                   verbose=1,\n",
        "                                                   save_best_only=True, save_weights_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtchdFIkTkP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02552062-56d4-4981-b848-daed84321397"
      },
      "source": [
        "nums_epoch = 30\n",
        "h = model.fit_generator(generator(), callbacks=[el_stop, reduce_lr, checkpointer], \n",
        "                        epochs=nums_epoch, steps_per_epoch=train_size // batch_size, \n",
        "                        validation_data = vaild_generator(), validation_steps = val_size//batch_size,\n",
        "                        use_multiprocessing = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize DataSet\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "initialize DataSet\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 175 steps, validate for 75 steps\n",
            "Epoch 1/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 2.6302 - accuracy: 0.1859\n",
            "Epoch 00001: val_loss improved from inf to 2.40859, saving model to model_zero7.01-2.628163.hdf5\n",
            "175/175 [==============================] - 879s 5s/step - loss: 2.6282 - accuracy: 0.1864 - val_loss: 2.4086 - val_accuracy: 0.2621\n",
            "Epoch 2/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 2.2617 - accuracy: 0.3111\n",
            "Epoch 00002: val_loss improved from 2.40859 to 2.15698, saving model to model_zero7.02-2.260736.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 2.2607 - accuracy: 0.3111 - val_loss: 2.1570 - val_accuracy: 0.3492\n",
            "Epoch 3/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 2.0090 - accuracy: 0.4018\n",
            "Epoch 00003: val_loss improved from 2.15698 to 1.97048, saving model to model_zero7.03-2.008777.hdf5\n",
            "175/175 [==============================] - 868s 5s/step - loss: 2.0088 - accuracy: 0.4020 - val_loss: 1.9705 - val_accuracy: 0.4129\n",
            "Epoch 4/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.8186 - accuracy: 0.4686\n",
            "Epoch 00004: val_loss improved from 1.97048 to 1.82032, saving model to model_zero7.04-1.818611.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.8186 - accuracy: 0.4682 - val_loss: 1.8203 - val_accuracy: 0.4583\n",
            "Epoch 5/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.6629 - accuracy: 0.5205\n",
            "Epoch 00005: val_loss improved from 1.82032 to 1.69803, saving model to model_zero7.05-1.663152.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.6632 - accuracy: 0.5198 - val_loss: 1.6980 - val_accuracy: 0.4817\n",
            "Epoch 6/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.5379 - accuracy: 0.5577\n",
            "Epoch 00006: val_loss improved from 1.69803 to 1.60518, saving model to model_zero7.06-1.537406.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.5374 - accuracy: 0.5573 - val_loss: 1.6052 - val_accuracy: 0.5008\n",
            "Epoch 7/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.4301 - accuracy: 0.5903\n",
            "Epoch 00007: val_loss improved from 1.60518 to 1.51999, saving model to model_zero7.07-1.429711.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.4297 - accuracy: 0.5904 - val_loss: 1.5200 - val_accuracy: 0.5267\n",
            "Epoch 8/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.3365 - accuracy: 0.6164\n",
            "Epoch 00008: val_loss improved from 1.51999 to 1.45332, saving model to model_zero7.08-1.336830.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.3368 - accuracy: 0.6159 - val_loss: 1.4533 - val_accuracy: 0.5492\n",
            "Epoch 9/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.2601 - accuracy: 0.6410\n",
            "Epoch 00009: val_loss improved from 1.45332 to 1.40255, saving model to model_zero7.09-1.260911.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 1.2609 - accuracy: 0.6402 - val_loss: 1.4026 - val_accuracy: 0.5650\n",
            "Epoch 10/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.1937 - accuracy: 0.6537\n",
            "Epoch 00010: val_loss improved from 1.40255 to 1.35266, saving model to model_zero7.10-1.194492.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.1945 - accuracy: 0.6529 - val_loss: 1.3527 - val_accuracy: 0.5713\n",
            "Epoch 11/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.1418 - accuracy: 0.6668\n",
            "Epoch 00011: val_loss improved from 1.35266 to 1.31448, saving model to model_zero7.11-1.141638.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.1416 - accuracy: 0.6666 - val_loss: 1.3145 - val_accuracy: 0.5813\n",
            "Epoch 12/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.0951 - accuracy: 0.6771\n",
            "Epoch 00012: val_loss improved from 1.31448 to 1.27890, saving model to model_zero7.12-1.095962.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 1.0960 - accuracy: 0.6762 - val_loss: 1.2789 - val_accuracy: 0.5888\n",
            "Epoch 13/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.0420 - accuracy: 0.6932\n",
            "Epoch 00013: val_loss improved from 1.27890 to 1.24839, saving model to model_zero7.13-1.042646.hdf5\n",
            "175/175 [==============================] - 867s 5s/step - loss: 1.0426 - accuracy: 0.6930 - val_loss: 1.2484 - val_accuracy: 0.5996\n",
            "Epoch 14/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 1.0160 - accuracy: 0.7085\n",
            "Epoch 00014: val_loss improved from 1.24839 to 1.22530, saving model to model_zero7.14-1.016177.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 1.0162 - accuracy: 0.7084 - val_loss: 1.2253 - val_accuracy: 0.6054\n",
            "Epoch 15/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 0.9622 - accuracy: 0.7141\n",
            "Epoch 00015: val_loss improved from 1.22530 to 1.20089, saving model to model_zero7.15-0.962199.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 0.9622 - accuracy: 0.7139 - val_loss: 1.2009 - val_accuracy: 0.6075\n",
            "Epoch 16/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 0.9445 - accuracy: 0.7189\n",
            "Epoch 00016: val_loss improved from 1.20089 to 1.18140, saving model to model_zero7.16-0.944356.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 0.9444 - accuracy: 0.7191 - val_loss: 1.1814 - val_accuracy: 0.6083\n",
            "Epoch 17/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 0.9280 - accuracy: 0.7189\n",
            "Epoch 00017: val_loss improved from 1.18140 to 1.16477, saving model to model_zero7.17-0.928336.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 0.9283 - accuracy: 0.7184 - val_loss: 1.1648 - val_accuracy: 0.6146\n",
            "Epoch 18/30\n",
            "174/175 [============================>.] - ETA: 3s - loss: 0.8949 - accuracy: 0.7286\n",
            "Epoch 00018: val_loss improved from 1.16477 to 1.15113, saving model to model_zero7.18-0.895249.hdf5\n",
            "175/175 [==============================] - 866s 5s/step - loss: 0.8952 - accuracy: 0.7275 - val_loss: 1.1511 - val_accuracy: 0.6217\n",
            "Epoch 19/30\n",
            " 29/175 [===>..........................] - ETA: 8:36 - loss: 0.8813 - accuracy: 0.7166"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyGkhtL1sML-"
      },
      "source": [
        "test_dataset1 = test_ds.repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "def test_generator():\n",
        "    while True:\n",
        "      print(\"initialize DataSet\")\n",
        "      for inputs, output in test_dataset1:\n",
        "          yield inputs, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g14Wb1QNZRp1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "cbacf540-9a2f-4df7-d81c-f5809904725d"
      },
      "source": [
        "model.evaluate_generator(test_generator(), steps=TEST_DATA_SIZE//batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-41-3f11a14e19cc>:1: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.evaluate, which supports generators.\n",
            "initialize DataSet\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8839845983755021, 0.71949404]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}